## 리뷰
- 월요일 오전 강의에서, Spark Optimization을 배웠다. 개발자의 입장에서 할 수 있는 Optimization은 결국 논리적 실행 계획인 DAG를 최적화한다는 데 있다는 것을 이해했다.
- Data Warehouse, Data Lake, Lake House의 차이점을 이해했다. 데이터가 분산 저장된 경우, shuffle이 가장 큰 병목 현상임을 이해했다.
- Parquet의 장점(읽는 속도가 빠르다), cache vs persist vs checkpoint의 차이점, data skew(데이터 불균형) 시 해결 방법으로 파티셔닝, Salting, Broadcasting, filtering 방법이 있다는 걸 이해했다.
- spark가 spark memory보다 더 큰 사이즈의 메모리를 필요로 하는 데이터를 불러오려면 Spill을 통해 메모리 밖의 디스크를 활용한다는 점을 찾아 봤다. 다만 속도는 떨어진다고 한다.
- 프로젝트 주제 선정을 위한 iteration 규칙을 정하고, time boxing을 도입해 실행에 옮겼다.
- Dano님이 iteration을 최대한 많이 돌리는 게 좋다고 해서 해봤는데, 처음엔 반신반의했지만 내가 낸 아이디어가 별로라는 걸 빠르게 알 수 있다는 점에서 장점이 있었다. 다만, iteration을 계속 돌린다고 해서 프로젝트 주제의 해답을 찾을 수 있을지는 아직 모르겠다. 일단 iteration을 여러 번 돌리는 과정을 계속 진행해봐야겠다.
## 회고
- Keep: 우리 팀이 하나가 되어 프로젝트를 완수하기 위해, 1 on 1 제도를 도입했다. 팀원들과 1:1로 대화하며 최대한 이성적으로 우리 팀이 소통이 잘 되지 못했던 점을 솔직하게 이야기했다. 팀원과 나의 다른 점을 이해할 수 있었고, 앞으로 팀이 어떻게 나아가야 할지 파악할 수 있었다.
